\documentclass[10pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amssymb}

\usepackage{geometry}
\geometry{left=1.0cm}
\geometry{right=1.0cm}
\geometry{top=1.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

\begin{document}
\section{Титульный слайд}
Всем добрый день, тема моей диссертационной работы ``Априорное распределение параметров в задачах выбора моделей глубокого обучения''.

\section{Априорное распределение параметров моделей}
В работе исследуются методы задания априорного распределения параметров моделей глубокого обучения с учетом накопленной информации о решаемой задаче.

Задача является актуальной задачей в связи с ростом параметров в моделях глубокого обучения. Увеличение числа параметров, необходимое для повышения качества аппроксимации, влечет повышение вычислительной сложности алгоритмов оценки параметров модели. В случае задание априорного распределения на параметрах позволяет строить модель в пространствах параметров меньшей размерности.

Снижение же размерности пространства параметров моделей глубокого обучения при незначительной потере качества позволяет использовать модели глубокого обучения на устройствах с низкой производительностью, в частности, на мобильных устройствах.

В рамках диссертационной работы предлагается использовать методы связного байесовского вывода. А, для назначения априорного распределения параметров при выборе моделей используется априорная экспертная информация и распределение параметров ранее полученных моделей.

\section{Привилегированное обучение В. Н. Вапника и дистилляция Дж. Хинтона}
Значимая часть результатов посвящена снижению пространства параметров на основе дистилляции моделей глубокого обучения.
Обобщенная постановка дистилляции, предложенная Владимиром Наумовичем Вапником и Джефри Хинтоном следующая: пусть задано признаковое описание модели ученика, которые далее будем называть просто признаки и признаковое описание учителя.

В рамках постановки Вапника предполагалось, что множество объектов для модели ученика и учителя совпадает, а различие только в признаковом описании этих объектов. В рамках диссертационной работы данное предположение не вводится, поэтому дополнительно вводится индексное множество объектов для которых привилегированные признаки известны.

Далее предполагается, что модель $\mathbf{f}$ учителя задана и требуется выбрать модель ученика $\mathbf{g}$ из некоторого множества~$\mathfrak{G}$ минимизируя функционал~$\mathcal{L}$.

\section{Байесовская дистилляция модели}
В базовой постановке задачи машинного обучения не учитывается никакой дополнительной, экспертной информации о рассматриваемой задаче. Это соответствует верхней части диаграммы.

Заданы только: модель являющаяся некоторым параметрическим семейством, выборка, которую требуется аппроксимировать и функционал качества, согласно которому требуется выборка оптимальный вектор параметров~$w$. И в случае вероятностной постановки, дополнительные параметры распределения.

В случае использования базовой дистилляции моделей машинного обучения дополнительно рассматривается новая переменная~$s$~--- ответы модели учителя. В байесовской же дистилляции параметры априорного распределения модели ученика получены из апостериорного распределения параметров модели учителя.

В обоих дистилляционных постановках, как в классической, так и в байесовской требуется минимизировать некоторый функционал качества с некоторой вариацией, которая выделена цветом. К примеру в базовой дистилляции упускается априорное распределение~$\mathbf{w}_0, \mathbf{A}_0$ выделенное зеленым цветом, а в байесовской дистилляции упускаются ответы учителя~$\mathbf{s}$ выделенные синим цветом.

\section{Оптимизация модели ученика на основе учителя}
Для удобства предположим, что признаково описание объектов для модели учителя и ученика совпадает. Задано признаковое описание объектов~$\mathbf{x}_i$ из~$\mathbb{R}^{n}.$ В качестве целевой переменной рассматриваются метки классов объектов. Параметрические семейства учителя и ученика заданы в виду суперпозиции некотрой диффиренцируемой функции~$\mathbf{v}, \mathbf{z}$ и функции~$\text{softmax}$.

Функция ошибки для минимизации состоит из двух слагаемых. Первое слагаемое это кросс-энтропийная, исходная, функция потерь для задачи классификации. Второе же слагаемое является слагаемым дистилляции, которое введено Джеффри Хинтоном эмпирически.

Именно второе слагаемое дистилляции представляет основной интерес для дальнейшего исследования и обоснования.

\section{Вероятностная постановка задачи дистилляции}
В рамках диссертационной работы представлена следующая вероятностная интерпретация дистилляции.

Пусть задано распределение целевой переменной~$y$, а также задано совместное распределение целевой переменной~$y$ с ответами модели учителя~$s$. Предполагается, что модель учителя является ``адекватной'', то есть целевая переменная~$y$ и предсказания модели являются зависимыми величинами с положительной ко-вариацией. Причем, в случае, если предсказания модели учителя отсутствует для всех объектов, то правдоподобие модели должно совпадать с правдоподобием модели без дистилляции.

Учитывая гипотезы~1-4 получено совместное правдоподобие истинных меток и меток учителя. Иммено это правдоподобие требуется максимизировать. Для удобства решения задачи оптимизации рассматривается логарифм от исходной функции правдоподобия.

Заметим, что целевая переменная и ответы модели зависят только посредством признакового описания~$x$, а следовательно получаем эквивалентную задачу для оптимизации. Получаем аналогичное разделение функции ошибки на слагаемые. Первые два слагаемых это слагаемые правдоподобия выборки, а последнее слагаемое это слагаемое дистилляции.

Заметим, что метапараметр~$\lambda$ был введен только для перевзвешивания слагаемых дистилляции и правдоподобия выборки.

\section{Вероятностная постановка задачи классификации}
В случае задачи классификации, в качестве распределения истинных меток  работе рассмотрено категориальное распределение, а в качестве распределения ответов учителя рассмотрено распределение которое задается своей плотностью. В диссертационной работы мною была доказана теорема, что предложенная функция действительно задает плотность распределения.

При использовании таких распределений, получаем оптимизационную задачу эквивалентную оптимизационной задаче предложенною в работах Джеффри Хинтона полученную эмпирическим путем.

Получаем, что введенные в диссертации гипотезы порождения данных является адекватной и позвволяет обобщить ранее эмпирически полученные слагаемые дисстиляции.

\section{Вероятностная постановка задачи регрессии}

Рассмотрим случай, когда распределения истинных меток и распределение меток учителя из нормальных распределений, а аппроксимирующая функция является линейной моделью.

Для задачи регрессии в рамках диссертационной работы доказано, что при данных ограничениях дистилляция эквивалентна простой задаче линейной регрессии с точностью до переобозначения. А именно переопределяется дисперсия объектов и целевая переменная для всех объектов для которых известно предсказания учителя.

Сами преобразования представлены внизу слайда (показать). 

\section{Байесовская постановка задачи дистилляции}
Вероятностная дистилляция, как и классическая дистилляция не учитывают информацию о параметрах модели учителя и ученика.

Рассмотрим частный случай параметрического семейства в котором модель ученика и учителя являются полносвязными нейронными сетями.

Далее матрицы и вектора~$U$ обозначают параметры учителя, а~$W$ обозначают параметры ученика.

Параметры модели ученика в рамках диссертационной работы выбираются на основе вариационного вывода. Для его использования требуется априорное распределение параметров модели ученика. Соответственно предлагается метод, в рамках которого априорное распределение параметров ученика задается на основе апостериорного распределения параметров учителя.

Основная проблема данного подхода это то, что пространства параметров учителя и ученика в общем случае не совпадают.

\section{Выравнивание структур моделей}
В качестве структуры параметрической модели рассмотрим последовательность размерностей скрытых представлений после каждого слоя нейросетевой модели.

Выравниванием структур назовем изменение структуры одной или нескольких моделей в результате которого векторы параметров моделей лежат в одновм пространстве.


В случае, если структура модели учителя и ученика совпадает, то апостериорное распределение модели учителя назначается априорным распределением модели ученика.

В случае же, если структуры моделей отличаются, то сначала проводится их выравнивание.

\section{Размеры скрытых слоев учителя и ученика различны}
Предложен метод для выравнивания двух структур в случае, когда число слоев модели учителя и ученика совпадает, а различаются только размеры этих слоев.

Рассматривается отображение~$\varphi$, которое соответствуют удалению одного нейрона с~$t$-го слоя.

Доказана теорема о виде распределения параметров полносвязной нейросетевой модели после удаления нейрона.

Также доказана теорема-следствие о том, что если до удаления нейрона вектор параметров модели являлся нормально распределенным, то после удаления одного нейрона распределение также будет нормальным.

Причем в рамках доказательства второй теоремы приводится явное построение результирующего нормального распределения.

\section{Решение задачи выравнивания структур моделей}

Проилюстрируем идею удаления одного нейрона. Рассмотрим модель полносвязной сети~$f$. В процессе выравнивания, параметры модели условно делятся на~$3$ типа: удаляемые, зануляемые, оставшиеся.

Не умоляя общности пусть модель учителя и ученика отличается только размерностью~$t$-го слоя. Причем, размерность слоя ученика на единицу меньше чем соответствующий слой модели учителя. Также не умоляя общности пусть требуется удалить~$j$-ю строку матрицы~$U_t$ --- для удобства на слайде она выделена красным цветом.

Удаление~$j$-й строки матрицы~$U_t$ эквивалентно занулению~$j$-го столбца матрицы~$U_{t+1}$, в следствии чего прогноз модели учителя не учитывает эту строку при вычислении прогноза.

С другой стороны нулевые значения в~$j$-м столбце матрицы~$U_{t+1}$ позволяют принимать произвольные значения в~$j$-й строке матрицы~$U_{t}$ не влияющие на предсказания модели.

Эти два факт приводят к простому выполнению двух действий: подсчета условного распределения при занулении части параметров и маргинализации другой части параметров.

Далее из свойств нормального распределения получаем параметры распределения вектора параметров~$\upsilon$.

\section{Число скрытых слоев учителя и ученика различны}

В рамках диссертационной работы также предложен метод для выравнивания двух структур в случае, когда число слоев у модели учителя и модели ученика различны.

Вводится отображение пси, которое соответствует удалению одного слоя из нейросетевой модели.
Доказана теорема, которая гласит, что в случае если: исходный вектор параметров распределён нормально, удаляемый слой задается квадратной матрицей, а также что функции активации удовлетворяют свойству идемпотентность, то распределение параметров модели после удаления слоя также будет нормальным распределением, с плотностью указанной в теореме.

\section{Обобщение для рекурентной сети RNN}

Представленные выше теоремы имеют обобщение и для других структур нейросетевых моделей. К примеру для рекурентных сетевых структур. Структура рекурентнной сети также задается последовательностью размерностей скрытых слоев.

Соответствующие теоремы были доказаны и для рекурентных сетей. Как для удаления нейронов со скрытых слоев, так и для удаления слоев.

\section{Последовательность выравнивающих преобразований}

Предыдущие теоремы описывают некоторое локальное преобразование, которое отображает одну структуру в новую структуру либо с меньшим числом нейронов либо с меньшим числов слоев.

Для выравнивания двух заданынх структур требуется провести последовательность таких локальных преобразований, которые описаны выше.

Рассмотрим множество всех структур, которые описываются последовательностью натуральных чисел. А также вводиться множество структур, которые порождаются структурой модели учителя~$\mathbf{f}$.

Следующая теорема указывает ограничения на множество структур, которые могут быть получены на основе одной структуры учителя.

Важно, что теорема доказывает существования такой последовательности, но легко показать, что такая последовательностью не единственная.

Рассмотрим пример показан на рисунке. Из структуры модели учителя~$\mathbf{f}$ в структуру ученика~$\mathf{g}$ существует три различных последовательности преобразований.

\section{Введение отношения порядка на множестве параметров}
На предыдущих двух слайдах не был оговорен выбор нейронов или слоев для удаления. В рамках диссертационной работы были проведены исследования следующих методов по заданию порядка на множестве параметров:
\begin{itemize}
    \item случайный порядок, который являлся базовым методом задания порядка;
    \item метод оптимального прореживания задает порядок на основе коэффициентов при квадратичном члене в разложении Тейлора функции ошибки~$L$ по параметрам модели учителя~$u$;
    \item метод на основе анализа апостеориорного распределения параметров в котором анализируется отношения апостериорной вероятности в нуле к апостеориорной вероятности значения параметра;
\end{itemize}
А также предложенные методы
\begin{itemize}
    \item на основе анализа мультиколиниарности параметров модели методом Белсли;
    \item на основе анализа ковариационной матрицы градиентов параметров модели для функции ошибки~$L$;
\end{itemize}

На графике видно, что к примеру порядок заданный методом Белсли позволяет удалять больше параметров без значимой потери качества.

\section{Анализ вероятностных свойств ответов модели ученика}
Вычислительные эксперименты в диссертационной работе показывают, что правдоподобие модели ученика в случае использования байесовской дистилляции растет много быстрее чем правдоподобие модели ученика без использования дистилляции. В эксперименте используется модель учителя с 3мя слояами, а модель ученика с 2мя слоями.

Для численной характеристики качества дистилляции в диссертационной работе вводится интегральный критерий, который имеет простую интерпретацию: площадь между графиками. Например разность площадей между синим и красным графиком. В таблице представлены результаты сравнения площадей для разных дистиллированных моделей. Положительное значение указывает на эффективность дистилляции. Показано, что правдоподобие модели ученика на основве байесовской дистилляции растет быстрее чем правдоподобие модели ученика на основе базовой дистилляции Хинтона.

В рамках диссертационной работы проведен ряд вычислительных экспертов, которые посвящены вероятностному анализу ответов моделей. Показано, что дистилляция позволяет учесть ``истинные'' распределения над метками классов.

В таблице приведены результаты вычислительного эксперимента для некоторых выборок. Подробный анализ приведен для синтетической выборки, так как о ней известна информация об истинных вероятностях классов для каждого объекта. Видно, что кросс-энтропийная ошибка с метками меньше у модели ученика без учителя, но если рассматривать кросс-энтропию между предсказанными вероятностями и реальными вероятностями, то ошибка меньше у модели с учителем.

Также видно, что модель без учителя имеет большее различие в средней разности наиболее правдоподобной метки класса и наименее правдоподобной метки класса, что свидетельствует о ее переобучении.

\section{Выносится на защиту}
В рамках диссертационной работы предложен байесовский метод выбора моделей ученика используя накопленную информацию об исследованной задачи.

Также были доказаны теоремы об эквивалентности для дистилляции моделей в случае задачи регрессии и классификации

Доказаны теоремы о виде априорного распределения параметров модели ученика для байесовской дистилляции.

Отдельной частью диссертации является исследования методов задания порядка на множестве параметров для параметрических моделей. Были предложены методы на основе анализа мультиколлиниарности параметров, а также на основе анализа ковариационной матрицы градиентов параметров.

\section{Список работ автора по теме диссертации}
Список публикаций и выступлений на конференциях по теме диссертационной работе представлен на слайде.

\end{document}