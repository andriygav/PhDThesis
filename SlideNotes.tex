\documentclass[10pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage[english,russian]{babel}

\usepackage{geometry}
\geometry{left=1.0cm}
\geometry{right=1.0cm}
\geometry{top=1.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

\begin{document}
\section{Главный слайд}
Добрый день, тема моей диссертации ``Априорное распределение параметров в задачах выбора моделей глубокого обучения''.

\section{Априорное распределение параметров моделей}
В работе исследуются методы задания априорного распределения параметров модели глубокого обучения с учетом накопленной информации о решаемой задаче.

Основные проблемы при построение моделей глубокого обучения это высокая размерность пространства параметров этих моделей. Данное свойство влечет многоэкстремальность и невыпуклость задачи оптимизации. Использовании априорного распределения параметров позволяет локализовать область поиска оптимальных параметров.

Для решения задачи выбора моделей предлагается использовать методы связного байесовского вывода. Для назначения априорного распределения параметров при выборе моделей глубокого обучения используется априорная экспертная информация и распределение параметров ранее полученных моделей.

\section{Оптимизация ученика на основе учителя и привилегированных признаков}
Обобщенная постановка дистилляции, предложенная Владимиром Наумовичем Вапником следующая: пусть задано признаковое описание модели ученика, которые далее будем называть просто признаки и признаковое описание учителя, которое будем называть привилегированные признаки. В рамках постановки Вапника предполагалось, что множество объектов для модели ученика и учителя совпадает. В рамках диссертационной работы данное предположение было упущено, поэтому дополнительно введем индексное множество объектов для которых привилегированные признаки известны.

Далее предполагается, что модель учителя задана и требуется выбрать модель ученика из некоторого множества минимизируя функционал~$\mathcal{L}$.

\section{Базовая постановка задачи машинного обучения}
В базовой же постановке задачи машинного обучения не присутствует никакой дополнительной, экспертной информации о рассматриваемой задаче. Заданы только: модель являющаяся некоторым параметрическим семейством, выборка, которую требуется аппроксимировать и функционал качества, согласно которому требуется выборка оптимальный вектор параметров~$w.$ И в случае вероятностной постановки, дополнительные параметры распределения.

\section{Априорное распределение параметров}
Простым решением для задания априорного распределения параметров является задание априорного распределение параметров модели на основе экспертной информации о рассматриваемой задачи.

\section{Байесовская дистилляция модели}
В рамках диссертационной работы, мною предложена обобщенная постановка, в рамках которой априорное распределение параметров модели можно рассматривать как функцию от параметров ранее предобученной модели.

В рамках этой постановке мною введено ограничение, в рамках которого в качестве модели учителя рассматривается только вероятностная модель учителя.

\section{Привилегированное обучение В. Н. Вапника и дистилляция Дж. Хинтона}
Начнем из классической постановки Вапника и Хинтона и их основных отличий, которые на данном слайде выделены цветом. Основное отличие заключается в том, что в рамках работы Хинтона предполагается, что признаковое описание для модели учителя и ученика совпадает. В рамках постановки Вапника это условие не выполняется. Также заметим, что в рамках их работ рассматривается только задача классификации.

Функция ошибки для минимизации условно может быть разделена на два слагаемых. Первое слагаемое это просто кросс-энтропийная исходная функция потерь для задачи классификации. Второе слагаемое является слагаемым дистилляции.

Второе слагаемое дистилляции представляет основной интерес для дальнейшего вероятностного исследования.

\section{Вероятностная постановка задачи дистилляции}
В рамках диссертационной работы вводится следующая вероятностная интерпретация дистилляции.

Пусть задано распределение целевой переменной~$y$, а также задано совместное распределение целевой переменной~$y$ с ответами модели учителя~$s$. В диссертации предполагается, что модель учителя является ``адекватной'', то есть целевая переменная~$y$ и предсказания модели являются зависимыми величинами. 

Далее вводится простое ограничение, которое говорит, что в случае, если объекты с ответами учителя не заданы, то задача должна быть эквивалентна классической задачи без дистилляции.

Учитывая гипотезы~1-4 автором работы введено совместное правдоподобие истинных меток и меток учителя. Далее требуется максимизировать записанное правдоподобие. 

Взяв логарифм от функции правдоподобия, а также взяв во внимания, что целевая переменная и ответы модели зависят только посредством признаков~$x$ получаем эквивалентную задачу для оптимизации. Заметим, что метапараметр~$\lambda$ в диссертации был введен только для перевзвешивания слагаемых дистилляции.

\section{Вероятностная постановка задачи классификации}
В случае задачи классификации, в качестве распределения истинных меток рассмотрим категориальное распределение, а в качестве распределения ответов учителя рассмотрим распределение которое задается своей плотностью.
В рамках диссертационной работы доказана теорема, которая доказывает, что предложенная функция действительно задает плотность распределения и как следствии данной теоремы получаем оптимизационную задачу эквивалентную оптимизационной задаче предложенною в работах Хинтона и Вапника.

Получаем, что введенные в диссертации гипотезы порождения данных является адекватной.

\section{Вероятностная постановка задачи регрессии}

Для задачи регрессии в рамках диссертационной работы доказано, что в случае, если распределения истинных меток и распределение меток учителя из нормальных распределений с некоторым параметрами. То дистилляция эквивалентна простой задаче линейной регрессии с небольшими переобозначениями, которые указаны внизу слайда: а именно переопределяется дисперсия объектов и целевая переменная для всех объектов для которых известны привелигированные признаки.

\section{Байесовская постановка задачи дистилляции}
Вероятностная дистилляция, как и классическая дистилляция имеет ряд недостатков связанных с тем, что не используется информация о параметрах модели учителя и ученика.

В рамках диссертационной работы рассмотривается частный случай параметрического семейства в котором модель ученика и учителя являются полносвязными нейронными сетями.

Далее матрицы и вектора U обозначают параметры учителя, а W обозначают параметры ученика.

Параметры модели ученика в рамках диссертационной работы выбираются на основе вариационного вывода. Для использования вариационного вывода требуется требуется априорное распределение параметров ученика. В рамках диссертационной работы предлагается метод, в рамках которого априорное распределение параметров ученика задается  на основе апостериорного распределения параметров учителя.

Основная проблема данного подхода это то, что пространства параметров учителя и ученика могут не совпадать.

\section{Сопоставление структурных параметров}
В случае, если структура модели учителя и ученика совпадает. То есть число слоев и размеры соответствующих слоев совпадает, то апостериорное распределение модели учителя просто назначается априорным распределением модели ученика.

В случае, если структуры моделей отличаются, то сначала проводится сопоставление двух структур.

\section{Размеры скрытых слоев учителя и ученика отличаются}
В рамках диссертационной работы предложен метод для сопоставления двух структур в случае, когда число слоев модели учителя и ученика совпадает, а различаются только размеры скрытых слоев.

Мною вводится отображение фи, которое выполняет удаление одного нейрона с t-го слоя.
Мною доказана теорема о том, что если до удаления нейрона вектор параметров модели являлся нормально распределенным, то после удаления одного нейрона распределение также будет нормальным. Причем в рамках доказательства этой теоремы приводится явная формула для построения этого распределения.

Замечу, что в случае, когда отличие в нескольких слоях, то теорема имеет простое обобщение и на этот случай.

\section{Решение задачи сопоставления структур моделей}

Идея сопоставления структур очень проста и может быть легко визуализирована. Рассмотрим некоторую модель перцептрона~$f$. В рамках диссертации, в процессе сопоставления параметры модели условно делятся на~$3$ типа: удаляемые, зануляемые, оставшиеся.

Пусть модель учителя и ученика отличается только размерностью~$t$-го слоя. Причем, размерность слоя ученика на единицу меньше чем модели учителя. Не умоляя общности представим, что для сопоставления требуется удалить~$j$-ю строку матрицы~$U_t$.

Удаление~$j$-й строки матрицы~$U_t$ можно добиться занулением~$j$-го столбца матрицы~$U_{t+1}$, что позволит не учитывать эту строку при вычислении прогноза.

С другой стороны, нулевые значения в~$j$-м столбце матрицы~$U_{t+1}$ позволяют принимать произвольные значения в~$j$-й строке матрицы~$U_{t}$.

Этот факт в рамках диссертационной работы сводится к простому выполнению двух действий: подсчета условного распределения и маргинализации.

Далее из свойств нормального распределения мною легко получены параметры апостериорного распределения после сопоставления параметров.

\section{Число скрытых слоев учителя и ученика различны}
В рамках диссертационной работы также предложен метод для сопоставления двух структур в случае, когда число слоев у модели учителя и модели ученика разные.
Мною вводится отображение пси, которое выполняет удаление одного слоя.
Мною доказана теорема о том, что при выполнении ряда ограничений на модель, таких как вектор параметров распределён нормально, удаляемый слой является квадратной матрицей, а также что функции активации удолетворяют свойству идемпотентность, то распределение параметров модели после удаления слоя также будет нормальным распределением, с заданной плотностью.

Замечу, что в случае, когда отличие в нескольких слоях, то теорема имеет простое обобщение и на этот случай.

\section{Введение отношения порядка на множестве параметров}
На предыдущих двух слайдах не был оговорен выбор нейронов или слоев для удаления. В рамках диссертационной работы мною были проведены исследования следущих методов по заданию порядка на множестве параметров:
\begin{itemize}
    \item случайный порядок, который являлся базовым методом задания порядка
    \item метод оптимального прореживания задает порядок на основе коэффициентов при квадратичном члене в разложении Тейлора функции ошибки~$L$ по параметрам модели учителя~$u$.
    \item метод на основе анализа апостеориорного распределения параметров в котором анализируется отношения апостериорной вероятности в нуле к апостеориорной вероятности значения параметра.
    \item метод на основе анализа мультиколиниарности параметров модели методом Белсли.
    \item метод на основе анализа ковариационной матрицы градиентов параметров модели для функции ошибки~$L$.
\end{itemize}
\section{Анализ вероятностных свойств ответов модели ученика}
В рамках диссертационной работы проведен ряд вычислительных экспертов, которые посвящены вероятностному анализу ответов моделей. Показано, что дистилляция позволяет учесть ``истинные'' распределения над метками классов.

То есть, рассмотрим некоторые объекты для которых задано истинное распределение меток для некоторых объектов. В то время, когда модель без учителя переобучается на конкретные классы, что характеризуется близкими к единице значений вероятностей предсказанных меток. Модель с учителем имеет более сглаженные вероятности классов, что соответствует истинному распределению меток. То есть получаем, что дистилляция является в некотором роде регуляризация модели, что было видно из вида функции ошибки.

В таблице приведены результаты вычислительного эксперимента для некоторых выборок. Подробный анализ приведен для синтетической выборки, так как о ней известна полная информация. Видно, что кросс-энтропийная ошибка с метками меньше у модели ученика без учителя, но если рассматривать кросс-энтропию между предсказанными вероятностями и реальными вероятностями, то ошибка меньше у модели с учителем.

Также видно, что модель без учителя имеет большее различие в разности вероятностей между наиболее правдоподобной меткой и наименее чем модель с учителем.

\section{Анализ правдоподобия выборки, сопоставлении моделей с разным числом скрытых слоев}

Вычислительные эксперименты в диссертационной работе показывают, что правдоподобие модели ученика в случае использования байесовской дистилляции растет много быстрее чем правдоподобие модели ученика без использования дистилляции.

В качестве примера рассматривается пример учителя с 3мя слоями, а в качестве ученика рассматривается полносвязный перцептрон с 2мя слоями.

Для численной характеристики качества дистилляции мною в диссертационной работе вводится интегральный критерий, который имеет простую интерпретацию: площадь между графиками. Например разность площадей между синим и красным графиком. В таблице представлены результаты сравнения площадей для разных дистиллированных моделей. Положительное значение указывает на эффективность дистилляции;

\section{Выносится на защиту}
В рамках диссертационной работы мною предложен байесовский метод выбора моделей ученика используя накопленную информацию об исследованной задачи. В частности ранее обученные модели.

Также мною были доказан теоремы об эквивалентности для дистилляции моделей в случае задачи регрессии и классификации. Доказаны теоремы о виде априорного распредлеения параметров модели ученика для байесовской дистилляции.

Отдельной частью диссертации является исследования методов задания порядка на множестве параметров для параметрических моделей. Мною было предложены методы на основе анализа мультиколлиниарности параметров, а также на основе анализа ковариационной матрицы градиентов параметров.

\section{Список работ автора по теме диссертации}
Список публикаций и выступлений на конференциях по теме диссертационной работе представлен на слайде. Всем спасибо.

\end{document}