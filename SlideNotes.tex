\documentclass[10pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amssymb}

\usepackage{geometry}
\geometry{left=1.0cm}
\geometry{right=1.0cm}
\geometry{top=1.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.}

\begin{document}
\section{Титульный слайд}
Всем добрый день, тема моей диссертационной работы ``Априорное распределение параметров в задачах выбора моделей глубокого обучения''.

\section{Априорное распределение параметров моделей}
В работе исследуются методы задания априорного распределения параметров с учетом накопленной информации о решаемой задаче.

Задача является актуальной в связи с ростом числа параметров в моделях глубокого обучения. Увеличение их числа необходимо для повышения качества аппроксимации, но оно влечет повышение вычислительной сложности аппроксимации. В свою очередь задание априорного распределения на параметрах модели, позволяет работать с пространствами меньшей размерности без потери качества.

Снижение размерности пространства параметров при незначительной потере качества делает возможным использовать эти модели на устройствах с низкой производительностью, в частности, на мобильных устройствах.

Для назначения априорного распределения параметров предлагается использовать априорная экспертную информацию и апостериорное распределение параметров ранее обученных моделей.

\section{Привилегированное обучение В. Н. Вапника и дистилляция Дж. Хинтона}
Значимая часть результатов посвящена снижению размерности пространства параметров используя дистилляцию.

Обобщенная постановка задачи дистилляции следующая: пусть заданы признаковые описание модели ученика и учителя, а также целевая переменная.

Пусть привилегированная информация доступна не для всех объектов, а только для тех, которые содержаться в индексном множестве~$\mathcal{I}$.

Предполагается, что модель учителя задана, а требуется выбрать модель ученика из некоторого множества, минимизируя функционал ошибки~$\mathcal{L}$.

\section{Байесовская дистилляция модели}
В базовой постановке задачи машинного обучения не учитывается дополнительной информации о рассматриваемой задаче. Это соответствует верхней части диаграммы.

Задана только модель из некоторого параметрического семейством; выборка, которую требуется аппроксимировать; и функционал качества, согласно которому выбирается оптимальный вектор параметров~$\mathbf{w}$.

В случае использования базовой дистилляции рассматривается новая переменная~$s$ описывающая ответы модели учителя. В байесовской же дистилляции, параметры априорного распределения ученика получены из апостериорного распределения учителя.

В обоих дистилляция, как в классической, так и в байесовской требуется минимизировать функционал качества, но с некоторым отличием, которое выделено цветом. К примеру в базовой дистилляции упускается априорное распределение параметров~$\mathbf{w}_0$ и $\mathbf{A}_0$ выделенное зеленым цветом. А в байесовской дистилляции упускаются ответы учителя~$\mathbf{s}$ выделенные синим цветом.

\section{Оптимизация модели ученика на основе учителя}
Предположим, что признаковое описание объектов для учителя и ученика совпадает. Пусть задано признаковое описание объектов~$\mathbf{x}_i$ из~$\mathbb{R}^{n}.$ В качестве целевой переменной рассматриваются метки классов. Параметрические семейства учителя и ученика задаются в виде суперпозиции диффиренцируемых функций~$\mathbf{v}$ и $\mathbf{z}$ с функцией~$\text{softmax}$ соотвественно.

Функция ошибки для минимизации состоит из двух слагаемых. Первое слагаемое это кросс-энтропийная функция потерь для задачи классификации. Второе же слагаемое является слагаемым дистилляции введенное Джеффри Хинтоном эмпирически.

Второе слагаемое дистилляции представляет основной интерес для дальнейшего исследования.

\section{Вероятностная постановка задачи дистилляции}
В рамках диссертационной работы предложена следующая вероятностная интерпретация дистилляции.

Пусть задано распределение целевой переменной, а также задано совместное распределение этой переменной с ответами модели учителя. Предполагается, что модель учителя является ``адекватной'', то есть целевая переменная и предсказания модели являются зависимыми величинами с положительной ко-вариацией. Причем, в случае, если предсказания модели учителя отсутствует для всех объектов, то правдоподобие модели должно совпадать с правдоподобием модели без дистилляции.

Учитывая гипотезы, получено совместное правдоподобие истинных меток и меток учителя. Именно это правдоподобие требуется максимизировать. Для удобства решения задачи оптимизации рассматривается логарифм от совместного правдоподобия.

Заметим, что целевая переменная и ответы учителя зависят только посредством признакового описания, а следовательно получаем эквивалентную задачу для оптимизации. Функция ошибки состоит из трех слагаемых. Первые два слагаемых это слагаемые правдоподобия выборки, а последнее слагаемое это слагаемое дистилляции.

\section{Вероятностная постановка задачи классификации}
Пусть распределение истинных меток является категориальным, а ответов учителя задается своей плотностью. Доказана теорема что предложенная функция действительно задает плотность распределения.

При использовании таких распределений, получаем оптимизационную задачу эквивалентную оптимизационной задаче эмпирически предложенной Джеффри Хинтоном.

Получаем, что введенные гипотезы порождения данных является адекватными и позволяют обобщить ранее эмпирически полученные слагаемые дистилляции.

\section{Вероятностная постановка задачи регрессии}
Рассмотрим случай, когда распределения истинных меток и меток учителя является нормальным. А аппроксимирующие функции линейными.

Для задачи регрессии доказано, что при этих ограничениях дистилляция эквивалентна простой задаче линейной регрессии с точностью до переобозначения. А именно переопределяется дисперсия и значение целевой переменной для всех объектов для которых известно предсказания учителя.

Сами преобразования представлены внизу слайда (показать). 

\section{Байесовская постановка задачи дистилляции}
Классическая и вероятностная дистилляции не учитывают информацию о параметрах учителя.

Рассмотрим частный случай параметрического семейства, в котором ученик и учитель являются полносвязными нейронными сетями.

Далее матрицами и векторами~$U$ обозначим параметры учителя, а~$W$ обозначим параметры ученика.

Параметры ученика выбираются на основе вариационного вывода. Но, для его использования требуется априорное распределение параметров ученика.

Предлагается метод, в котором априорное распределение параметров ученика задается на основе апостериорного распределения параметров учителя.

Основная проблема данного метода заключается в том, что пространства параметров учителя и ученика в общем случае не совпадают.

\section{Выравнивание структур моделей}
В качестве структуры параметрической модели рассмотрим последовательность размерностей скрытых представлений после каждого слоя нейросетевой модели.

Выравниванием структур назовем изменение структуры одной или нескольких моделей в результате которого вектора параметров лежат в одном пространстве.

В случае, если структура учителя и ученика совпадает, то апостериорное распределение параметров учителя назначается априорным распределением параметров ученика.

В случае же, если структуры моделей отличаются, то сначала проводится их выравнивание.

\section{Размеры скрытых слоев учителя и ученика различны}
Предложен метод для выравнивания двух структур в случае, когда число слоев учителя и ученика совпадает, а различаются только размеры этих слоев.

Рассматривается отображение, которое соответствуют удалению одного нейрона с~$t$-го слоя.

Доказана теорема о виде распределения параметров полносвязной нейросетевой модели после удаления одного нейрона.

Также доказана теорема-следствие о том, что если до удаления нейрона вектор параметров имел нормальное распределение, то после удаления одного нейрона распределение также будет нормальным.

\section{Решение задачи выравнивания структур моделей}
Сама теорема имеет простую иллюстрацию. Рассмотрим модель полносвязной сети~$f$. В процессе выравнивания, параметры модели условно делятся на три типа: удаляемые, зануляемые и оставшиеся.

Пусть модель учителя и ученика отличается только размерностью~$t$-го слоя. Причем, размерность слоя ученика на единицу меньше чем соответствующий слой учителя. Также не умоляя общности пусть требуется удалить~$j$-ю строку матрицы~$U_t,$ которая выделена красным цветом.

Удаление~$j$-й строки матрицы~$U_t$ эквивалентно занулению~$j$-го столбца матрицы~$U_{t+1}$, в следствии чего модель учителя не учитывает эту строку при вычислении прогноза.

С другой стороны нулевые значения в~$j$-м столбце матрицы~$U_{t+1}$ позволяют принимать произвольные значения в~$j$-й строке матрицы~$U_{t}$ не влияющие на предсказания модели.

Эти факты приводят к простому выполнению двух действий: подсчета условного распределения при занулении части параметров и маргинализации другой части параметров.

Далее из свойств нормального распределения получаем распределение вектора параметров после удаления одного нейрона.

\section{Число скрытых слоев учителя и ученика различны}
Предложен метод для выравнивания двух структур в случае, когда число слоев у учителя и ученика различны.

Вводится отображение, которое соответствует удалению одного слоя из нейросетевой модели.
Доказана теорема, которая гласит, что в случае если исходный вектор параметров распределён нормально, удаляемый слой задается квадратной матрицей, а функция активации удовлетворяет свойству идемпотентность, то распределение параметров после удаления слоя также будет нормальным.

\section{Обобщение для рекурентной сети RNN}
Представленные выше теоремы имеют обобщение и для других структур нейросетевых моделей. На слайде представлены соответсвующие теоремы для рекурентных сетей. Как для удаления нейронов со скрытых слоев, так и для удаления слоев. Структура рекурентнной сети в этом случае задается последовательностью размерностей скрытых слоев.

\section{Последовательность выравнивающих преобразований}
Предыдущие теоремы описывают локальное преобразование нейросетей, которое отображает исходную структуру в новую с отличием в одном нейроне либо в одном слое.

Для выравнивания двух заданных структур, требуется провести последовательность таких преобразований.

Рассмотрим множество всех структур, которые описываются последовательностью натуральных чисел. А также введем множество структур, которые порождаются структурой модели учителя~$\mathbf{f}$.

Теорема указывает ограничения на множество структур, которые могут быть получены на основе структуры учителя.

Важно, что теорема доказывает существования последовательности преобразований, но легко показать, что такая последовательностью не единственная.

Рассмотрим пример представленный на рисунке. Из структуры учителя в структуру ученик существует три различных последовательности преобразований.

\section{Введение отношения порядка на множестве параметров}
На предыдущих слайдах не был оговорен выбор нейронов или слоев для удаления. Проведены исследования различных методов по заданию порядка на множестве параметров:
\begin{itemize}
    \item это случайный порядок;
    \item это метод оптимального прореживания;
    \item и метод на основе анализа апостеориорного распределения параметров;
\end{itemize}
А также были предложены новые методы
\begin{itemize}
    \item на основе анализа мультиколиниарности параметров методом Белсли;
    \item и на основе анализа ковариационной матрицы градиентов параметров;
\end{itemize}

На графике видно, что порядок заданный методом Белсли позволяет удалять больше параметров без значимой потери качества.

\section{Анализ вероятностных свойств ответов модели ученика}
Вычислительные эксперименты показывают, что правдоподобие ученика в случае использования байесовской дистилляции растет много быстрее чем правдоподобие ученика без использования дистилляции.

Для численной характеристики качества дистилляции введен интегральный критерий, имеющий простую интерпретацию: площадь между графиками. Положительное значение указывает на эффективность дистилляции.

В таблице представлены результаты сравнения интегрального критерия для разных дистилляций. Показано, что правдоподобие ученика на основе байесовской дистилляции растет быстрее чем правдоподобие ученика на основе базовой дистилляции.

Проведен эксперимент по вероятностному анализу ответов моделей. Показано, что дистилляция позволяет учесть распределения над метками классов. Видно, что кросс-энтропийная ошибка меньше у модели ученика без учителя. Но, кросс-энтропия между предсказанными и реальными вероятностями меньше у модели с учителем.

Модель без учителя имеет бОльшую разность между наиболее и наименее правдоподобными метками классов, что свидетельствует о ее переобучении.

\section{Выносится на защиту}
В рамках диссертационной работы предложен байесовский метод выбора моделей ученика используя накопленную информацию об исследованной задачи.

Доказаны теоремы об эквивалентности для дистилляции моделей в случае задачи регрессии и классификации

Доказаны теоремы о виде априорного распределения параметров ученика для байесовской дистилляции.

Отдельной частью диссертации является исследования методов задания порядка на множестве параметров нейросетей. Предложены методы на основе анализа мультиколлиниарности, а также на основе анализа ковариационной матрицы градиентов.

\section{Список работ автора по теме диссертации}
Список публикаций и выступлений на конференциях по теме диссертационной работе представлен на слайде.
\end{document}