\documentclass[10pt, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage[english,russian]{babel}

\usepackage{geometry}
\geometry{left=1.0cm}
\geometry{right=1.0cm}
\geometry{top=1.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

\begin{document}
\section{Главный слайд}
Добрый день, тема моей диссертационной работы ``Априорное распределение параметров в задачах выбора моделей глубокого обучения''.

\section{Априорное распределение параметров моделей}
В работе исследуются методы задания априорного распределения параметров модели глубокого обучения с учетом накопленной информации о решаемой задаче.

Основные проблемы при построение моделей глубокого обучения это высокая размерность пространства параметров этих моделей. Данное свойство влечет многоэкстремальность и невыпуклость задачи оптимизации. В свою очередь использование априорного распределения параметров позволяет локализовать область поиска оптимальных параметров.

Для решения задачи выбора моделей предлагается использовать методы связного байесовского вывода. А, для назначения априорного распределения параметров при выборе моделей используется априорная экспертная информация и распределение параметров ранее полученных моделей.

\section{Привилегированное обучение В. Н. Вапника и дистилляция Дж. Хинтона}
В рамках диссертационной работы значимая часть результатов посвящена снижению пространства параметров на основе дистилляции моделей глубокого обучения.
Обобщенная постановка дистилляции, предложенная Владимиром Наумовичем Вапником и Джефри Хинтоном следующая: пусть задано признаковое описание модели ученика, которые далее будем называть просто признаки и признаковое описание учителя, которое будем называть привилегированные признаки. В рамках постановки Вапника предполагалось, что множество объектов для модели ученика и учителя совпадает, а различие только в признаковом описании этих объектов.

В рамках диссертационной работы данное предположение не вводится, поэтому дополнительно вводится индексное множество объектов для которых привилегированные признаки известны.

Далее предполагается, что модель учителя задана и требуется выбрать модель ученика из некоторого множества минимизируя функционал~$\mathcal{L}$.

\section{Базовая постановка задачи машинного обучения}
В базовой же постановке задачи машинного обучения не учитывается никакой дополнительной, экспертной информации о рассматриваемой задаче. Заданы только: модель являющаяся некоторым параметрическим семейством, выборка, которую требуется аппроксимировать и функционал качества, согласно которому требуется выборка оптимальный вектор параметров~$w.$ И в случае вероятностной постановки, дополнительные параметры распределения.

\section{Априорное распределение параметров}
Простым решением для задания априорного распределения параметров является задание априорного распределение параметров модели на основе экспертной информации о рассматриваемой задачи.

Например, то что параметры распределены нормально с некоторыми параметрами~$w, A$.

\section{Байесовская дистилляция модели}
В рамках диссертационной работы, предложена обобщенная постановка, в рамках которой априорное распределение параметров~$w$ модели ученика~$g$ рассматривается как функция от параметров~$u$ ранее полученной модели~$f$.

В рамках этой постановке введено ограничение, в рамках которого в качестве модели~$f$ рассматривается только вероятностная модель.

\section{Оптимизация модели ученика на основе учителя~и привилегированных признаков}
Классическая постановка дистилляции имеет вариативность. Данная вариативность на слайде выделена цветом. Также заметим, что в классических работ рассматривается только задача классификации, что видно из вида параметрического семейства

Функция ошибки для минимизации условно может быть разделена на два слагаемых. Первое слагаемое это просто кросс-энтропийная, исходная, функция потерь для задачи классификации. Второе слагаемое является слагаемым дистилляции.

Второе слагаемое дистилляции представляет основной интерес для дальнейшего вероятностного исследования.

\section{Вероятностная постановка задачи дистилляции}
В рамках диссертационной работы вводится следующая вероятностная интерпретация дистилляции.

Пусть задано распределение целевой переменной~$y$, а также задано совместное распределение целевой переменной~$y$ с ответами модели учителя~$s$. В диссертации предполагается, что модель учителя является ``адекватной'', то есть целевая переменная~$y$ и предсказания модели являются зависимыми величинами. 

Далее вводится простое ограничение, которое говорит, что в случае, если объекты с ответами учителя не заданы, то задача должна быть эквивалентна классической задачи без дистилляции.

Учитывая гипотезы~1-4 введено совместное правдоподобие истинных меток и меток учителя. Далее требуется максимизировать записанное правдоподобие. 

Взяв логарифм от функции правдоподобия, а также приняв во внимания, что целевая переменная и ответы модели зависят только посредством признаков~$x$, то получаем эквивалентную задачу для оптимизации. Первое слагаемое это слагаемое правдоподобия выборки, а второе слагаемое это слагаемое дистилляции.

Заметим, что метапараметр~$\lambda$ был введен только для перевзвешивания слагаемых дистилляции и правдоподобия выборки.

\section{Вероятностная постановка задачи классификации}
В случае задачи классификации, в качестве распределения истинных меток  работе рассмотрено категориальное распределение, а в качестве распределения ответов учителя рассмотрено распределение которое задается своей плотностью.
В диссертационной работы доказана теорема, которая доказывает, что предложенная функция действительно задает плотность распределения и как следствии данной теоремы получаем оптимизационную задачу эквивалентную оптимизационной задаче предложенною в работах Хинтона и Вапника.

Получаем, что введенные в диссертации гипотезы порождения данных является адекватной.

\section{Вероятностная постановка задачи регрессии}

Для задачи регрессии в рамках диссертационной работы доказано, что в случае, если распределения истинных меток и распределение меток учителя из нормальных распределений, то дистилляция эквивалентна простой задаче линейной регрессии с точностью до переобозначения, которое указано внизу слайда: а именно переопределяется дисперсия объектов и целевая переменная для всех объектов для которых известны привелигированные признаки.

\section{Байесовская постановка задачи дистилляции}
Вероятностная дистилляция, как и классическая дистилляция имеет ряд недостатков связанных с тем, что не используется информация о параметрах модели учителя и ученика.

В рамках диссертационной работы рассматривается частный случай параметрического семейства в котором модель ученика и учителя являются полносвязными нейронными сетями.

Далее матрицы и вектора U обозначают параметры учителя, а W обозначают параметры ученика.

Параметры модели ученика в рамках диссертационной работы выбираются на основе вариационного вывода. Для использования вариационного вывода требуется априорное распределение параметров модели ученика. Предлагается метод, в рамках которого априорное распределение параметров ученика задается  на основе апостериорного распределения параметров учителя.

Основная проблема данного подхода это то, что пространства параметров учителя и ученика в общем случае не совпадают.

\section{Выравнивание структурных параметров}
В случае, если структура модели учителя и ученика все же совпадает. Другими словами число слоев и размеры соответствующих слоев совпадает, то апостериорное распределение модели учителя просто назначается априорным распределением модели ученика.

В случае же, если структуры моделей отличаются, то сначала проводится выравнивание этих структур.

\section{Размеры скрытых слоев учителя и ученика отличаются}
В рамках диссертационной работы предложен метод для выравнивания двух структур в случае, когда число слоев модели учителя и ученика совпадает, а различаются только размеры этих слоев.

Рассматривается отображение фи, которое выполняет удаление одного нейрона с t-го слоя.
Доказана теорема о том, что если до удаления нейрона вектор параметров модели являлся нормально распределенным, то после удаления одного нейрона распределение также будет нормальным. Причем в рамках доказательства этой теоремы приводится явное построение этого распределения.

Замечу, что в случае, когда отличие в нескольких слоях, то теорема имеет простое обобщение и на этот случай.

\section{Решение задачи выравнивания структур моделей}

Идея выравнивания структур очень проста и может быть легко визуализирована. Рассмотрим некоторую модель полносвязной сети~$f$. В рамках диссертации, в процессе выравнивания параметры модели условно делятся на~$3$ типа: удаляемые, зануляемые, оставшиеся.

Пусть модель учителя и ученика отличается только размерностью~$t$-го слоя. Причем, размерность слоя ученика на единицу меньше чем соответствующий слой модели учителя. Не умоляя общности представим, что для выравнивания требуется удалить~$j$-ю строку матрицы~$U_t$.

Удаление~$j$-й строки матрицы~$U_t$ можно добиться занулением~$j$-го столбца матрицы~$U_{t+1}$, что позволит не учитывать эту строку при вычислении прогноза моделью учителя.

Нулевые значения в~$j$-м столбце матрицы~$U_{t+1}$ позволяют принимать произвольные значения в~$j$-й строке матрицы~$U_{t}$.

Эти два факт в рамках диссертационной работы приводят к простому выполнению двух действий: подсчета условного распределения при занулении части параметров и маргинализации.

Далее из свойств нормального распределения выше описанная теорема легко доказывается.

\section{Число скрытых слоев учителя и ученика различны}
В рамках диссертационной работы также предложен метод для выравнивания двух структур в случае, когда число слоев у модели учителя и модели ученика разные.
Вводится отображение пси, которое выполняет удаление одного слоя.
Доказана теорема о том, что при выполнении ряда ограничений на модель: вектор параметров распределён нормально, удаляемый слой задается квадратной матрицей, а также что функции активации удовлетворяют свойству идемпотентность, то распределение параметров модели после удаления слоя также будет нормальным распределением, с заданной плотностью.

Замечу, что в случае, когда отличие в нескольких слоях, то теорема также имеет простое обобщение и на этот случай.

\section{Введение отношения порядка на множестве параметров}
На предыдущих двух слайдах не был оговорен выбор нейронов или слоев для удаления. В рамках диссертационной работы были проведены исследования следующих методов по заданию порядка на множестве параметров:
\begin{itemize}
    \item случайный порядок, который являлся базовым методом задания порядка
    \item метод оптимального прореживания задает порядок на основе коэффициентов при квадратичном члене в разложении Тейлора функции ошибки~$L$ по параметрам модели учителя~$u$.
    \item метод на основе анализа апостеориорного распределения параметров в котором анализируется отношения апостериорной вероятности в нуле к апостеориорной вероятности значения параметра.
    \item метод на основе анализа мультиколиниарности параметров модели методом Белсли.
    \item метод на основе анализа ковариационной матрицы градиентов параметров модели для функции ошибки~$L$.
\end{itemize}
\section{Анализ вероятностных свойств ответов модели ученика}
В рамках диссертационной работы проведен ряд вычислительных экспертов, которые посвящены вероятностному анализу ответов моделей. Показано, что дистилляция позволяет учесть ``истинные'' распределения над метками классов.

То есть, рассмотрим некоторые объекты для которых задано истинное распределение меток для некоторых объектов. В то время, когда модель без учителя переобучается на конкретные классы, что характеризуется близкими к единице значений вероятностей предсказанных меток. Модель с учителем имеет более сглаженные вероятности классов, что соответствует истинному распределению меток. То есть получаем, что дистилляция является в некотором роде регуляризация модели, что было видно из вида функции ошибки.

В таблице приведены результаты вычислительного эксперимента для некоторых выборок. Подробный анализ приведен для синтетической выборки, так как о ней известна полная информация. Видно, что кросс-энтропийная ошибка с метками меньше у модели ученика без учителя, но если рассматривать кросс-энтропию между предсказанными вероятностями и реальными вероятностями, то ошибка меньше у модели с учителем.

Также видно, что модель без учителя имеет большее различие в средней разности наиболее правдоподобной меткой и наименее чем модель с учителем.

\section{Анализ правдоподобия выборки, выравнивание моделей с разным числом скрытых слоев}

Вычислительные эксперименты в диссертационной работе показывают, что правдоподобие модели ученика в случае использования байесовской дистилляции растет много быстрее чем правдоподобие модели ученика без использования дистилляции. В эксперименте используется модель учителя с 3мя слояами, а модель ученика с 2мя слоями.

Для численной характеристики качества дистилляции в диссертационной работе вводится интегральный критерий, который имеет простую интерпретацию: площадь между графиками. Например разность площадей между синим и красным графиком. В таблице представлены результаты сравнения площадей для разных дистиллированных моделей. Положительное значение указывает на эффективность дистилляции;

\section{Выносится на защиту}
В рамках диссертационной работы предложен байесовский метод выбора моделей ученика используя накопленную информацию об исследованной задачи. В частности ранее обученные модели.

Также были доказаны теоремы об эквивалентности для дистилляции моделей в случае задачи регрессии и классификации. Доказаны теоремы о виде априорного распределения параметров модели ученика для байесовской дистилляции.

Отдельной частью диссертации является исследования методов задания порядка на множестве параметров для параметрических моделей. Были предложены методы на основе анализа мультиколлиниарности параметров, а также на основе анализа ковариационной матрицы градиентов параметров.

\section{Список работ автора по теме диссертации}
Список публикаций и выступлений на конференциях по теме диссертационной работе представлен на слайде. Всем спасибо.

\end{document}