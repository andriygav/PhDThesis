\newpage


\section{Введение}
В силу высокой вычислительной сложности, время оптимизации нейронных сетей может занимать до нескольких дней \cite{sutskever2014}.
Построение и выбор оптимальной структуры нейронной сети также является вычислительно сложной процедурой, которая значимо влияет на итоговое качество модели. 
При этом алгоритмы оптимизации сходится по большинству параметров сети уже после небольшого числа итераций \cite{Chunyan2016}.
Своевременное определение начала сходимости параметров позволит существенно снизить вычислительные затраты на обучение моделей с большим числом параметров.
Примерами моделей, с большим число параметров, являются AlexNet \cite{Krizhevsky2012}, VGGNet \cite{Simonyan2014}, ResNet \cite{Kaiming2015}, BERT \cite{Devlin2018, Vaswani2017}, mT5 \cite{Linting2021}, GPT3\cite{Brown2020} и другие.

Рост числа параметров моделей глубокого обучения влечет снижение интерпретируемости ответов этих моделей.
Первые упоминания о данной проблемы рассмотрены А.\,Г. Ивахненко \cite{Ivakhnenko1994}.
Проблема с неинтерпретируемыми моделями широко сейчас рассматривается в классе задач по adversarial attack \cite{Zheng2020}.

Другой проблемой моделей с большим числом параметров является высокие требования к вычислителю в момент предсказания. Использование избыточно сложных моделей с избыточным числом неинформативных параметров является препятствием для использования глубоких сетей на мобильных устройствах в режиме реального времени.
Для снижения числа параметров в литературе рассматривается метод дистилляции модели на основе предсказаний модели учителя \cite{Hinton2015, Vapnik2015, Lopez2016}.
Модель с большим числом параметров называется учитель. Модель учителя дистиллируется в модель с малым числом параметров, которая называется ученик.
Основные идеи, которые описывают дистилляцию моделей глубокого обучения предложены в работах Дж.\,Е. Хинтона и В.\,Н. Вапником \cite{Hinton2015, Vapnik2015, Lopez2016}.
Работы предлагают использовать предсказания модели учителя для повышения качестве модели ученика.
В работе \cite{Vapnik2015} В.\,Н. Вапником вводится понятие привилегированной информации, которое позволяет использовать дополнительную информацию о данных в момент обучения модели.
Работа \cite{Lopez2016} объединяет идеи дистилляции \cite{Hinton2015} с идеями привилегированного обучения \cite{Vapnik2015}, предложив метод дистилляции модели учителя с большим числом признаков в модель ученика с меньшим числом признаков.
В предложенном методе \cite{Lopez2016} решается двухэтапная задача. На первом этапе строится модель учителя с расширенным признаковым описанием.
На втором этапе обучается модель ученика в исходном признаковом описании используя дистилляцию \cite{Hinton2015}.
В работе Дж.\,Е. Хинтона \cite{Hinton2015} поставлено множество экспериментов по дистилляции моделей глубокого обучения для задачи классификации.
Один из экспериментов проводился на выборке MNIST \cite{mnist}, который показал, что предложенный дистилляции позволяет построить нейросетевую модель меньшей сложности на основе модели большей сложности.
Второй эксперимент показывал идею по дистилляции ансамбля моделей в одну нейросетевую модель для решения задачи распознания речи. В работе \cite{Hinton2015} проводится сравнение дистилляции с моделью смеси экспертов.
Дальнейшие работы по дистилляции моделей глубокого обучения рассматривают возможность использования информации о значения параметров модели учителя для оптимизации параметров модели ученика. Работа \cite{Zehao2017} предлагает метод neuron selectivity transfer, который минимизирует специальную функцию потерь. Данная функция основается на maximum mean discrepancy между выходами слоев модели учителя и модели ученика. В рамках вычислительного эксперимента сравнивалось качество базовой дистилляции с предложенным методов на примере выборок CIFAR \cite{cifar10} и ImageNet \cite{imagenet}.

Дистилляция моделей глубокого обучения работает в предположение, что архитектура модели ученика уже известная. Для выборка архитектуры модели ученика предлагается использовать методы прореживания нейросетевых моделей. Существует ряд подходов к построению оптимальной сети. В работах \cite{maclarin2015, luketina2015} предлагается использовать модель градиентного спуска для оптимизации сети. В \cite{molchanov2017} используются байесовские методы \cite{neal1995} оптимизации параметров нейронных сетей. Другим методом поиска оптимальной структуры является прореживание избыточно сложной модели \cite{cun1990, louizos2017, graves2011}. В работе \cite{cun1990} предлагается удалять наименее релевантные параметры на основе значений первой и второй производных функции ошибки. В \cite{grabovoy2019} предложен метод определения релевантности параметров аппроксимирующих моделей при помощи метода Белсли. Релевантность параметров в работе \cite{grabovoy2019} определяется на основе ковариационной матрицы параметров модели.
Другим примером задания порядка на множестве параметров служит $l_1$-регуляризация \cite{Tibshirani1996} и регуляризация ElasticNet \cite{Hastie2005} для линейных моделей.
Порядок, заданный на множестве значений коэффициентов регуляризации, индуцирует порядок на множестве признаковых описаний и указывает на важность признаков.
В случае нейросетей для регуляризации параметров используется метод исключения параметров \cite{srivastava2014, molchanov2017}.
Данный метод также задает порядок на множестве параметров модели.

Порядок на множестве параметров нейросети можно использовать не только для удаления неимение релевантных параметров, а и для фиксации параметров в процесе оптимизации параметров. Работе \cite{grabovoy2020} посвящена оптимизации структуры нейронной сети, а также выбору параметров, которые можно зафиксировать после некоторой итерации градиентного метода.