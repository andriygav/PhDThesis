Основные результаты диссертационной работы заключаются в следующем.

В главе 1 введены основные понятия, поставлены задачи выбора априорного распределения параметров моделей машинного обучения. Проанализированы методы дистилляции и привилегированного обучения предложенные Владимир Наумовичем Вапником и Джефри Хинтоном. Проанализированы методы задания порядка на множестве параметров нейросетевых моделей. Последние включают в себя как эвристические методы, так и методы, основанные на байесовском выводе и вероятностных предположениях о распределении параметров моделей.

В главе 2 были предложены методы обобщения дистилляции и привилегированного обучения на основе вероятностного подхода. Введены вероятностные предположения, описывающие дистилляцию моделей. В рамках данных вероятностных предположений проанализированы модели для задачи классификации и регрессии. Результат анализа сформулирован в виде  соответствующих \emph{теорем об эквивалентности}. Теорема для задачи регрессии показала, что обучение линейной регрессии с учителем эквивалентно замене обучающей выборки и вероятностных предположений о распределении истинных ответов. Для задачи классификации ответы учителя дают же дополнительную информацию. Она задается в виде распределения классов для каждого объекта из обучающей выборки. Для задачи классификации проведен вычислительный эксперимент. Из вычислительного эксперимента видно, что дистилляция влияет на распределение классов для каждого объекта. Вероятности классов для каждого объекта являются более разреженными, а не концентрируются в одном классе. Данное свойство представлено в синтетической выборке, так как она генерировалась с максимальной дисперсией в вероятностях классов.

В главе 3 был предложен байесовский метод для дистилляции моделей глубокого обучения на основе вариационного вывода. Дистилляция основывается на задании априорного распределения параметров модели ученика. Априорное распределение параметров модели ученика задается на основе апостериорного распределения параметров модели учителя. Представлен механизм выравнивавния структуры модели учителя в структуру модели ученика. Результаты анализа данного механизма сформулированы в рамках \emph{теорем о виде априорного распределения}. В теоремах получен вид распределения после выравнивания пространства параметров модели учителя к пространству параметров модели ученика.

В главе 4 предложены методы задания априорного распределения параметров локальных моделей в задаче обучения смеси экспертов. Рассмотрен подход задания гиперпараметров априорного распределения на основе экспертной информации о рассматриваемой задачи. Вводиться понятие регуляризации гиперпараметров априорных распределений согласно экспертной информации. Предлагается метод оптимизации гиперапарметров и параметров смеси экспертов в единой процедуре на основе ЕМ-алгоритма. Представлены результаты экспериментов по сравнению различных способов регуляризации гиперпараметров априорных распределений.

В главе 5 предложены методы введения отношения порядка на множестве параметров аппроксимирующих моделей. Введены вероятностные предположения в рамках которых решается поставленная задача. Предложен метод задания порядка на основе метода Белсли. Метод анализирует мультиколлинеарность параметров при обучении моделей глубокого обучения. Предложен метод задания порядка на основе анализа стохастических свойств градиента функции ошибки по параметрам модели. Данный метод использует ковариационную матрицу градиентов параметров. Предложенные методы проанализированы в вычислительном эксперименте. Показано, что предложенный порядок задает порядок на параметрах модели и соответствует влиянию параметров на функцию ошибки при оптимизации.

В главе 6 проведен анализ прикладных задач. Рассмотрена задача определения оптимального размера выборки. В ней задание экспертной информации о гипотезе порождения данных и классе аппроксимирующих моделей указывает достаточный объем выборки. Анализируется метод кластеризации точек квазипериодических временных рядов в случае, когда известна экспертная информация о структуре рассматриваемых рядов.