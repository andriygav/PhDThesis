Основные результаты диссертационной работы заключаются в следующем.

В главе 1 введены основные понятия, поставлены задачи выбора априорного распределения параметров моделей машинного обучения. Проанализированы методы дистилляции и привилегированного обучения предложенные Владимир Наумовичем Вапником и Джефри Хинтоном. Проанализированы методы задания порядка на множестве параметров нейросетевых моделей. Последние включают в себя как эвристические методы, так и методы, основанные на байесовском выводе и вероятностных предположениях о распределении параметров моделей.

В главе 2 были предложены методы обобщения дистилляции и привилегированного обучения на основе вероятностного подхода. Введены вероятностные предположения, описывающие дистилляцию моделей. В рамках данных вероятностных предположений проанализированы модели для задачи классификации и регрессии. Результат анализа сформулирован в виде  соответствующих \emph{теорем об эквивалентности}. Теорема для задачи регрессии показала, что обучение линейной регрессии с учителем эквивалентно замене обучающей выборки и вероятностных предположений о распределении истинных ответов. Для задачи классификации ответы учителя дают же дополнительную информацию в виде распределения классов для каждого объекта из обучающей выборки. Для задачи классификации проведен вычислительный эксперимент. Из вычислительного эксперимента видно, что дистилляция влияет на распределение классов в рамках одного объекта. Вероятности классов для каждого объекта являются более разреженными, а не концентрируются в одном классе. Данное свойство хорошо видно в синтетической выборке, так как она генерировалась с максимальной дисперсией в вероятностях классов.

В главе 3 был предложен байесовский подход для дистилляции моделей глубокого обучения на основе вариационного вывода. В рамках данного подхода дистилляция основывается на задании априорного распределения параметров модели ученика. Априорное распределение параметров модели ученика задается на основе апостериорного распределения параметров модели учителя. Представлен механизм преобразования структуры модели учителя в структуру модели ученика представлен. Результаты анализа данного механизма сформулированы в рамках \emph{теорем о виде априорного распределения}. Теоремы описывают механизм приведения пространства параметров модели учителя к пространству параметров модели ученика в случае, если число слоев совпадает, но размер слоев различается, также в случае если число слоев различается.

В главе 4 предложены методы задания априорного распределения параметров локальных моделей в задаче обучения смеси экспертов. Рассмотрен подход задания гиперпараметров априорного распределения на основе экспертной информации об рассматриваемой задачи. Вводиться понятия регуляризации гиперпараметров априорных распределения согласно экспертной информации для повышения качества аппроксимации выборки. Предлагается метод оптимизации гиперапарметров и параметров смеси экспертов в единой процедуре на основе ЕМ-алгоритма. Представлены результаты экспериментов по исследованию различных способов регуляризации гиперпараметров априорных распределений.

В главе 5 предложены методы введения отношения порядка на множестве параметров аппроксимирующих моделей. Введены вероятностные предположения в рамках которых решается поставленная задача. В рамках поставленной задачи предложен метод задания порядка на основе метода Белсли, который анализирует мультиколлинеарность обучающих параметров. Предложен метод задания порядка на основе анализа стохастических свойств градиента функции ошибки по параметрам модели. Данный метод использует ковариационную матрицу градиентов параметров. Предложенные методы проанализированы в вычислительному эксперименту. Показано, что предложенный порядок задает порядок на параметрах модели, который соответствует влиянию параметров на функцию ошибки, которая оптимизируется.

В главе 6 проведен анализ прикладных задач. Рассмотрена задача определения оптимального размера в которой задание экспертной информации о выборке и классе аппроксимирующих моделей позволяет указать требуемый объем выборки. Анализируется метод кластеризации точек квазипериодических временных рядов в случае, когда известна экспертная информация о структуре рассматриваемых рядов.