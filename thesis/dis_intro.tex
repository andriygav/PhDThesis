\textbf{Актуальность темы.}
Построение и выбор оптимальной структуры нейронной сети является вычислительно сложной процедурой~\cite{sutskever2014}, которая значимо влияет на итоговое качество модели. 
При этом большинство параметров модели перестают значимо изменяться уже после небольшого числа итераций алгоритма оптимизации~\cite{Chunyan2016}.
Своевременное определение начала сходимости параметров существенно снижает вычислительные затраты на обучение моделей с большим числом параметров.
Примерами таких моделей являются AlexNet~\cite{Krizhevsky2012}, VGGNet~\cite{Simonyan2014}, ResNet~\cite{Kaiming2015}, BERT~\cite{Devlin2018, Vaswani2017}, mT5~\cite{Linting2021}, GPT3~\cite{Brown2020}.
Рост числа параметров моделей глубокого обучения влечет снижение интерпретируемости ответов этих моделей~\cite{Ivakhnenko1994}.
Проблема с неинтерпретируемыми моделями рассматривается в классе задач по состязательным атакам~\cite{Zheng2020}.

Проблемой моделей с большим числом параметров является увеличение вычислительной сложности.
Использование избыточно сложных моделей с большим числом неинформативных параметров является препятствием для использования глубоких сетей на мобильных устройствах в режиме реального времени. \textit{Сложность модели} определяется числом настраиваемых параметров модели.
Для снижения числа параметров в литературе рассматривается метод дистилляции модели на основе предсказаний модели учителя~\cite{Hinton2015, Vapnik2015, Lopez2016}.
Сложная модель с большим числом параметров называется~\textit{учитель}. Модель учителя дистиллируется в менее сложную модель с малым числом параметров, которая называется~\textit{ученик}.
Методы дистилляции моделей глубокого обучения введены в работах Дж.\,Е. Хинтона и В.\,Н. Вапника~\cite{Hinton2015, Vapnik2015, Lopez2016}.
Предлагается использовать предсказания модели учителя для повышения качества ученика.
В~\cite{Vapnik2015} В.\,Н. Вапником вводится понятие привилегированной информации.
Работа~\cite{Lopez2016} объединяет идеи дистилляции~\cite{Hinton2015} с идеями привилегированного обучения~\cite{Vapnik2015}. В ней предлагается метод дистилляции учителя в модель ученика в случае, когда признаковое описания объектов не совпадает.
В~\cite{Lopez2016} решается двухэтапная задача. На первом этапе строится модель учителя с расширенным признаковым описанием.
На втором этапе при помощи дистилляции~\cite{Hinton2015} обучается ученик в исходном признаковом описании.
В работе Дж.\,Е. Хинтона~\cite{Hinton2015} поставлены эксперименты по дистилляции моделей глубокого обучения для задачи классификации.
Первый эксперимент анализирует выборку MNIST~\cite{mnist}. Он показывает, что предложенный метод дистилляции позволяет построить нейросетевую модель меньшей сложности на основе модели большей сложности.
Второй эксперимент анализирует метод дистилляции ансамбля моделей в одну нейросетевую модель для решения задачи распознания речи. В работе~\cite{Hinton2015} проводится сравнение дистилляции с моделью смеси экспертов.
Дальнейшие работы по дистилляции моделей глубокого обучения исследуют методы, использующие значения параметров модели учителя, для оптимизации параметров модели ученика. В~\cite{Zehao2017} предлагается метод передачи селективности~\cite{Tatarchuk2014} нейрона, минимизирующий специальную функцию потерь. Эта функция основывается на максимизация среднего описания между выходами слоев модели учителя и модели ученика. В рамках вычислительного эксперимента сравнивалось качество базовой дистилляции с предложенным методом на выборках CIFAR~\cite{cifar10} и ImageNet~\cite{imagenet}.

Дистилляция моделей глубокого обучения предполагает, что архитектура модели ученика уже известна. Для выбора архитектуры модели ученика предлагается использовать методы прореживания нейросетевых моделей. В работах~\cite{maclarin2015, luketina2015} предлагается использовать алгоритм градиентного спуска для оптимизации сети. В~\cite{molchanov2017} используются байесовские методы~\cite{neal1995} оптимизации параметров нейронных сетей. Существуют методы поиска оптимальной структуры используя удаления параметров сложной модели~\cite{cun1990, louizos2017, graves2011}. В работе~\cite{cun1990} предлагается удалять наименее~\textit{релевантные} параметры на основе значений первой и второй производных функции ошибки. В~\cite{grabovoy2019} предложен метод определения релевантности параметров аппроксимирующих моделей при помощи метода Белсли. \textit{Релевантность} параметров в работе~\cite{grabovoy2019} определяется на основе ковариационной матрицы параметров модели.
Другими примерами задания порядка на множестве параметров служат $l_1$-регуляризация~\cite{Tibshirani1996} и регуляризация ElasticNet~\cite{Hastie2005} для линейных моделей.
Порядок, заданный на множестве значений коэффициентов регуляризации, индуцирует порядок на множестве признаковых описаний и указывает на важность признаков.
В случае нейросетей для регуляризации параметров используется метод исключения параметров~\cite{srivastava2014, molchanov2017}.
Он также задает порядок на множестве параметров модели.

Порядок на множестве параметров нейросети используется не только для удаления наименее релевантных параметров, а и для фиксации параметров в процессе их оптимизации. Работа~\cite{grabovoy2020} посвящена оптимизации структуры нейронной сети, а также выбору параметров, которые фиксируются после некоторой итерации градиентного метода. 


\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Предложить байесовский метод выбора моделей с использованием модели учителя с привилегированной и накопленной информацией.
\item Предложить метод назначения априорного распределения параметров модели ученика с использованием апостериорного распределения параметров модели учителя.
\item Предложить вероятностную интерпретацию дистилляции моделей глубокого обучения.
\item Предложить метод использования экспертной информации о решаемой задаче прогнозирования при построении априорного распределения параметров.
\item Предложить метод назначения релевантности параметров моделей глубокого обучения для выбора модели машинного обучения.
\end{enumerate}

\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы вариационного байесовского вывода~\cite{mackay2002,bishop2006}, вероятностные~\cite{shiriyaev1980} методы анализа моделей глубокого обучения, статистические методы~\cite{kobzar2012,bishop2006} анализа распределений параметров моделей глубокого обучения.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
    \item Предложен байесовский метод выбора моделей с использованием модели учителя с привилегированной и накопленной информацией.
    \item Доказаны теоремы о свойствах дистилляции, 
    \begin{itemize}
        \item[---] \emph{теоремы об эквивалентности} для дистилляции моделей в случае задачи регрессии и классификации,
        \item[---] \emph{теоремы о виде априорного распределения} параметров модели ученика в байесовской дистилляции.
    \end{itemize}
    \item Предложен метод выравнивания вероятностных пространств параметров. Предложен метод выбора априорного распределения параметров модели ученика с использованием апостериорного распределения параметров модели учителя для случаев
    \begin{itemize}
        \item[---] различных размерностей пространств параметров отдельных слоев,
        \item[---] различного числа слоев нескольких моделей.
    \end{itemize}
    \item Предложены методы задания порядка на множестве параметров моделей
    \begin{itemize}
        \item[---] на основе корреляции параметров,
        \item[---] на основе оценки скорости сходимости параметров.
    \end{itemize}
    \item Предложена вероятностная интерпретации дистилляции моделей глубокого обучения. Исследованы свойства дистилляции моделей глубокого обучения.
\end{enumerate}

\vspace{0.5cm}
\textbf{Научная новизна.} Разработаны новые подходы к назначению априорного распределения параметров моделей. Предложен метод назначения априорного распределения используя экспертную информацию о задаче. Предложены методы задания порядка на множестве параметров нейросетевых моделей на основе анализа мультиколлиниорности параметров и скорости их сходимости. Предложено вероятностное обобщение дистилляции моделей. Предложено байесовское обобщение дистилляции моделей глубокого обобщения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} Диссертационная работа носит теоретический характер. В работе проводится теоретический анализ методов снижения размерности пространства параметров нейросетевых моделей. Доказаны~\emph{теоремы об эквивалентности} для дистилляции моделей в случае задачи регрессии и классификации. Доказаны~\emph{теоремы об априорном} распределения модели для байесовской дистилляции.

\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; снижения пространства параметров моделей глубокого обучения; использования экспертной информации для построения моделей; дистилляции параметрических моделей на основе выравнивания архитектур.

\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
    \item Задача обучения с экспертом для построения интерпретируемых моделей машинного обучения, Международная конференция <<Интеллектуализация обработки информации>>, 2020.
    \item Привилегированная информация и дистилляция моделей, Всероссийская конференция <<63-я научная конференция МФТИ>>, 2020.
    \item Введение отношения порядка на множестве параметров нейронной сети, Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2019.
    \item Анализ априорных распределений в задаче смеси экспертов, Всероссийская конференция <<62-я научная конференция МФТИ>>, 2019.
    \item Поиск оптимальной модели при помощи алгоритмов прореживания, Всероссийская конференция <<61-я научная конференция МФТИ>>, 2018.
    \item Автоматическое определение релевантности параметров нейросети, Международная конференция <<Интеллектуализация обработки информации>>, 2018.
\end{enumerate}

Работа поддержана грантами Российского фонда фундаментальных исследований:

\begin{enumerate}
    \item[1)] 19-07-00875, Развитие методов автоматического построения и выбора вероятностных моделей субоптимальной сложности в задачах глубокого обучения,
    \item[2)] 19-07-01155, Развитие теории порождения моделей локальной аппроксимации для классификации сигналов носимых устройств,
    \item[3)] 19-07-00885, Выбор моделей в задачах декодирования временных рядов высокой размерности.
\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 6 печатных изданиях в журналах, рекомендованных ВАК.
\begin{enumerate}
    \item \textit{Грабовой\;А.В., Стрижов\;В.В.} Байесовская дистилляция моделей глубокого обучения~// Автоматика и телемеханика ---~2021. ---~Т.~11. ---~С.~16--29.
    \item \textit{Грабовой\;А.В., Стрижов\;В.В.} Анализ выбора априорного распределения для смеси экспертов~// Журнал вычислительной математики и математической физики ---~2021. ---~Т.~61, №~7. ---~С.~1149--1161.
    \item \textit{Grabovoy\;A., Strijov\;V.} Quasi-periodic time series clustering for human // Lobachevskii Journal of Mathematics ---~2020. Vol.~41. ---~Pp.~333--339.
    \item \textit{Грабовой\;А.В., Бахтеев\;О.Ю., Стрижов\;В.В.} Введение отношения порядка на множестве параметров аппроксимирующих моделей~// Информатика и ее применения ---~2020. ---~Т.~14, №~2. ---~С.~58--65.
    \item \textit{Грабовой\;А.В., Бахтеев\;О.Ю., Стрижов\;В.В.} Определение релевантности параметров нейросети~// Информатика и ее применения ---~2019. ---~Т.~13, №~2. ---~С.~62--70.
    \item \textit{Грабовой\;А.В., Стрижов\;В.В.} Вероятностная интерпретация задачи дистилляции~// Автоматика и телемеханика ---~2022. ---~Т.~1. ---~С.~150--168.
\end{enumerate}

\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.

\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, шести разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из~\total{citnum} наименований. Основной текст занимает~\pageref{LastPage} страницы.

\vspace{0.5cm}
\textbf{Краткое содержание работы по главам.}
В главе 1 вводятся основные понятия, поставлены задачи выбора априорного распределения параметров моделей машинного обучения. Проанализированы методы дистилляции и привилегированного обучения предложенные Владимиром Наумовичем Вапником и Джефри Хинтоном. Анализируются существующие методы задания порядка на множестве параметров нейросетевых моделей.
 
В главе 2 предложены методы обобщения дистилляции и привилегированного обучения на основе вероятностного подхода.

В главе 3 предложен байесовский подход для дистилляции моделей глубокого обучения на основе вариационного вывода.

В главе 4 предложены методы задания априорного распределения параметров локальных моделей в задаче обучения смеси экспертов.

В главе 5 предложены методы введения отношения порядка на множестве параметров аппроксимирующих моделей.

В главе 6 проведен анализ прикладных задач, которые используют экспертную информацию.