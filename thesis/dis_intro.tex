\newpage{}
\addcontentsline{toc}{section}{Введение}
\chapter*{Введение}


\textbf{Актуальность темы.} В силу высокой вычислительной сложности, время оптимизации нейронных сетей может занимать до нескольких дней \cite{sutskever2014}.
Построение и выбор оптимальной структуры нейронной сети также является вычислительно сложной процедурой, которая значимо влияет на итоговое качество модели. 
При этом алгоритмы оптимизации сходится по большинству параметров сети уже после небольшого числа итераций \cite{Chunyan2016}.
Своевременное определение начала сходимости параметров позволит существенно снизить вычислительные затраты на обучение моделей с большим числом параметров.
Примерами моделей, с большим число параметров, являются AlexNet \cite{Krizhevsky2012}, VGGNet \cite{Simonyan2014}, ResNet \cite{Kaiming2015}, BERT \cite{Devlin2018, Vaswani2017}, mT5 \cite{Linting2021}, GPT3\cite{Brown2020} и другие.

Рост числа параметров моделей глубокого обучения влечет снижение интерпретируемости ответов этих моделей.
Первые упоминания о данной проблемы рассмотрены А.\,Г. Ивахненко \cite{Ivakhnenko1994}.
Проблема с неинтерпретируемыми моделями широко сейчас рассматривается в классе задач по adversarial attack \cite{Zheng2020}.

Другой проблемой моделей с большим числом параметров является высокие требования к вычислителю в момент предсказания. Использование избыточно сложных моделей с избыточным числом неинформативных параметров является препятствием для использования глубоких сетей на мобильных устройствах в режиме реального времени.
Для снижения числа параметров в литературе рассматривается метод дистилляции модели на основе предсказаний модели учителя \cite{Hinton2015, Vapnik2015, Lopez2016}.
Модель с большим числом параметров называется учитель. Модель учителя дистиллируется в модель с малым числом параметров, которая называется ученик.
Основные идеи, которые описывают дистилляцию моделей глубокого обучения предложены в работах Дж.\,Е. Хинтона и В.\,Н. Вапником \cite{Hinton2015, Vapnik2015, Lopez2016}.
Работы предлагают использовать предсказания модели учителя для повышения качестве модели ученика.
В работе \cite{Vapnik2015} В.\,Н. Вапником вводится понятие привилегированной информации, которое позволяет использовать дополнительную информацию о данных в момент обучения модели.
Работа \cite{Lopez2016} объединяет идеи дистилляции \cite{Hinton2015} с идеями привилегированного обучения \cite{Vapnik2015}, предложив метод дистилляции модели учителя с большим числом признаков в модель ученика с меньшим числом признаков.
В предложенном методе \cite{Lopez2016} решается двухэтапная задача. На первом этапе строится модель учителя с расширенным признаковым описанием.
На втором этапе обучается модель ученика в исходном признаковом описании используя дистилляцию \cite{Hinton2015}.
В работе Дж.\,Е. Хинтона \cite{Hinton2015} поставлено множество экспериментов по дистилляции моделей глубокого обучения для задачи классификации.
Один из экспериментов проводился на выборке MNIST \cite{mnist}, который показал, что предложенный дистилляции позволяет построить нейросетевую модель меньшей сложности на основе модели большей сложности.
Второй эксперимент показывал идею по дистилляции ансамбля моделей в одну нейросетевую модель для решения задачи распознания речи. В работе \cite{Hinton2015} проводится сравнение дистилляции с моделью смеси экспертов.
Дальнейшие работы по дистилляции моделей глубокого обучения рассматривают возможность использования информации о значения параметров модели учителя для оптимизации параметров модели ученика. Работа \cite{Zehao2017} предлагает метод neuron selectivity transfer, который минимизирует специальную функцию потерь. Данная функция основается на maximum mean discrepancy между выходами слоев модели учителя и модели ученика. В рамках вычислительного эксперимента сравнивалось качество базовой дистилляции с предложенным методов на примере выборок CIFAR \cite{cifar10} и ImageNet \cite{imagenet}.

Дистилляция моделей глубокого обучения работает в предположение, что архитектура модели ученика уже известная. Для выборка архитектуры модели ученика предлагается использовать методы прореживания нейросетевых моделей. Существует ряд подходов к построению оптимальной сети. В работах \cite{maclarin2015, luketina2015} предлагается использовать модель градиентного спуска для оптимизации сети. В \cite{molchanov2017} используются байесовские методы \cite{neal1995} оптимизации параметров нейронных сетей. Другим методом поиска оптимальной структуры является прореживание избыточно сложной модели \cite{cun1990, louizos2017, graves2011}. В работе \cite{cun1990} предлагается удалять наименее релевантные параметры на основе значений первой и второй производных функции ошибки. В \cite{grabovoy2019} предложен метод определения релевантности параметров аппроксимирующих моделей при помощи метода Белсли. Релевантность параметров в работе \cite{grabovoy2019} определяется на основе ковариационной матрицы параметров модели.
Другим примером задания порядка на множестве параметров служит $l_1$-регуляризация \cite{Tibshirani1996} и регуляризация ElasticNet \cite{Hastie2005} для линейных моделей.
Порядок, заданный на множестве значений коэффициентов регуляризации, индуцирует порядок на множестве признаковых описаний и указывает на важность признаков.
В случае нейросетей для регуляризации параметров используется метод исключения параметров \cite{srivastava2014, molchanov2017}.
Данный метод также задает порядок на множестве параметров модели.

Порядок на множестве параметров нейросети можно использовать не только для удаления неимение релевантных параметров, а и для фиксации параметров в процесе оптимизации параметров. Работе \cite{grabovoy2020} посвящена оптимизации структуры нейронной сети, а также выбору параметров, которые можно зафиксировать после некоторой итерации градиентного метода.

\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Предложить метод снижения размерности пространства параметров нейросетевых моделей.
\item Предложить метод сопоставления параметрических моделей на основе байесовского вывода.
\item Предложить байесовский метод обучения моделей ученика на основе моделей учителя.
\item Предложить метод выбора априорного распределения параметров модели используя экспертную информацию о задаче.
\end{enumerate}

\vspace{0.5cm}
\textbf{Методы исследования.} TODO

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
    \item Предложен байесовский метод обучения моделей ученика на основе моделей учителя.
    \item Предложен алгоритм построения модели на основе предобученных моделей.
    \item Предложен метод задания порядка на множестве параметров нейросетевых моделей на основе корреляции параметров.
    \item Предложен метод задания порядка на множестве параметров нейросетевых моделей при помощи оценки скорости сходимости параметров.
    \item Исследованы свойства дистилляции моделей глубокого обучения. Представлена вероятностная интерпретации дистиляции моделей глубокого обучения.
    \item Исследованы методы сопоставления параметрических моделей. Исследован метод получения априорного распределения параметров нейросети используя апостериорное распределение параметров исходной модели:
    \begin{itemize}
        \item различие в размерности слоев;
        \item различие в числе слоев.
    \end{itemize}
\end{enumerate}

\vspace{0.5cm}
\textbf{Научная новизна.} Разработаны новые подходы к заданию априорного распределения параметров моделей. Рассмотрен метод назначения априорного распределения используя экспертную информацию о задаче. Предложен метод автоматического назначения априорного распределения параметров моделей на основе предобученных параметрических моделей. Исследованы методы задания порядка на множестве параметров нейросетевых моделей на основе анализа мультиколлиниорности параметров и скорости их сходимости. Предложено вероятностное обобщение дистилляции моделей. Предложено байесовское обобщение дистилляции моделей глубокого обобщения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} В целом, данная диссертационная работа носит теоретический характер. В работе проводится теоретический анализ методов снижения размерности пространства параметров нейросетевых моделей. Доказаны ``теоремы об эквивалентности`` для дистилляции моделей в случае задачи регрессии и классификации. Доказана теорема об априорном распределения модели для байесовской дистилляции.

\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; снижения пространства параметров моделей глубокого обучения; использование экспертной информации для построения моделей; дистилляция параметрических моделей на основе сопоставления архитектур.

\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
    \item ``Автоматическое определение релевантности параметров нейросети'', Международная конференция <<Интеллектуализация обработки информации>>, 2018.
    \item ``Поиск оптимальной модели при помощи алгоритмов прореживания'', Всероссийская конференция <<61-я научная конференция МФТИ>>, 2018.
    \item ``Анализ априорных распределений в задаче смеси экспертов'', Всероссийская конференция <<62-я научная конференция МФТИ>>, 2019.
    \item ``Введение отношения порядка на множестве параметров нейронной сети'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2019.
    \item ``Привилегированная информация и дистилляция моделей'', Всероссийская конференция <<63-я научная конференция МФТИ>>, 2020.
    \item ``Задача обучения с экспертом для построение интерпретируемых моделей машинного обучения'', Международная конференция <<Интеллектуализация обработки информации>>, 2020.
\end{enumerate}

Работа поддержана грантами Российского фонда фундаментальных исследований.
\begin{enumerate}
    \item TODO
\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 5 печатных изданиях в журналах, рекомендованных ВАК.
\begin{enumerate}
\item Грабовой А.В., Бахтеев О.Ю., Стрижов В.В. ``Определение релевантности параметров нейросети'', Информатика и её применения. 13:2 (2019), 62--71.
\item Грабовой А.В., Бахтеев О. Ю., Стрижов В.В., ``Введение отношения порядка на множестве параметров аппроксимирующих моделей'', Информатика и ее применения. (2020).
\item A. Grabovoy, V. Strijov., ``Quasi-periodic time series clustering for human'', Lobachevskii Journal of Mathematics. (2020).
\item Грабовой А.В., Стрижов В.В., ``Анализ выбора априорного распределения для смеси экспертов'', Журнал Вычислительной математики и математической физики. (2021).
\item Грабовой А.В., Стрижов В.В., ``Байесовская дистилляция моделей глубокого обучения'', Автоматика и Телемеханика. (2021).
\end{enumerate}

\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.

\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из \total{citnum} наименований. Основной текст занимает \pageref{LastPage} страницы.

\vspace{0.5cm}
\textbf{Краткое содержание работы по главам.} TODO




