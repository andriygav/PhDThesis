\textbf{Актуальность темы.} В силу высокой вычислительной сложности, время оптимизации нейронных сетей занимает значительное время~\cite{sutskever2014}.
Построение и выбор оптимальной структуры нейронной сети является вычислительно сложной процедурой, которая значимо влияет на итоговое качество модели. 
При этом большинство параметров модели перестают значимо меняться уже после небольшого числа итераций алгоритма оптимизации~\cite{Chunyan2016}.
Своевременное определение начала сходимости параметров позволит существенно снизить вычислительные затраты на обучение моделей с большим числом параметров.
Примерами моделей, с большим число параметров, являются AlexNet~\cite{Krizhevsky2012}, VGGNet~\cite{Simonyan2014}, ResNet~\cite{Kaiming2015}, BERT~\cite{Devlin2018, Vaswani2017}, mT5~\cite{Linting2021}, GPT3~\cite{Brown2020} и другие.

Рост числа параметров моделей глубокого обучения влечет снижение интерпретируемости ответов этих моделей.
Первые упоминания этой проблемы рассмотрены А.\,Г. Ивахненко~\cite{Ivakhnenko1994}.
Проблема с неинтерпретируемыми моделями рассматривается в классе задач по состязательным атакам~\cite{Zheng2020}.

Проблемой моделей с большим числом параметров является увеличения вычислительной сложности в момент предсказания.
Использование избыточно сложных моделей с большим числом неинформативных параметров является препятствием для использования глубоких сетей на мобильных устройствах в режиме реального времени. \textit{Сложность модели} определяется числом настраиваемых параметров модели.
Для снижения числа параметров в литературе рассматривается метод дистилляции модели на основе предсказаний модели учителя~\cite{Hinton2015, Vapnik2015, Lopez2016}.
Сложная модель с большим числом параметров называется~\textit{учитель}. Модель учителя дистиллируется в менее сложную модель с малым числом параметров, которая называется~\textit{ученик}.
Методы дистилляции моделей глубокого обучения введены в работах Дж.\,Е. Хинтона и В.\,Н. Вапником~\cite{Hinton2015, Vapnik2015, Lopez2016}.
Предлагается использовать предсказания модели учителя для повышения качества ученика.
В~\cite{Vapnik2015} В.\,Н. Вапником вводит понятие привилегированной информации. Оно использует дополнительную информацию о данных в момент обучения модели.
Работа~\cite{Lopez2016} объединяет идеи дистилляции~\cite{Hinton2015} с идеями привилегированного обучения~\cite{Vapnik2015}, предложив метод дистилляции учителя в модель ученика в случае, когда признаковое описания объектов не совпадает..
В~\cite{Lopez2016} решается двухэтапная задача. На первом этапе строится модель учителя с расширенным признаковым описанием.
На втором этапе при помощи дистилляции~\cite{Hinton2015} обучается ученика в исходном признаковом описании.
В работе Дж.\,Е. Хинтона~\cite{Hinton2015} поставлены эксперименты по дистилляции моделей глубокого обучения для задачи классификации.
Первый эксперимент анализирует выборку MNIST~\cite{mnist}. Он показал, что предложенный метод дистилляции позволяет построить нейросетевую модель меньшей сложности на основе модели большей сложности.
Второй эксперимент анализирует метод дистилляции ансамбля моделей в одну нейросетевую модель для решения задачи распознания речи. В работе~\cite{Hinton2015} проводится сравнение дистилляции с моделью смеси экспертов.
Дальнейшие работы по дистилляции моделей глубокого обучения рассматривают возможность использования информации о значения параметров модели учителя для оптимизации параметров модели ученика. В~\cite{Zehao2017} предлагается метод передачи селективности~\cite{Tatarchuk2014} нейрона минимизирующий специальную функцию потерь. Эта функция основывается на максимиции среднего описания между выходами слоев модели учителя и модели ученика. В рамках вычислительного эксперимента сравнивалось качество базовой дистилляции с предложенным методов на примере выборок CIFAR~\cite{cifar10} и ImageNet~\cite{imagenet}.

Дистилляция моделей глубокого обучения предполагает, что архитектура модели ученика уже известна. Для выбора архитектуры модели ученика предлагается использовать методы прореживания нейросетевых моделей. В работах~\cite{maclarin2015, luketina2015} предлагается использовать алгоритма градиентного спуска для оптимизации сети. В~\cite{molchanov2017} используются байесовские методы~\cite{neal1995} оптимизации параметров нейронных сетей. Существуют методы поиска оптимальной структуры используя удаления параметров сложной модели~\cite{cun1990, louizos2017, graves2011}. В работе~\cite{cun1990} предлагается удалять наименее\textit{релевантные} параметры на основе значений первой и второй производных функции ошибки. В~\cite{grabovoy2019} предложен метод определения релевантности параметров аппроксимирующих моделей при помощи метода Белсли. \textit{Релевантность} параметров в работе~\cite{grabovoy2019} определяется на основе ковариационной матрицы параметров модели.
Другим примером задания порядка на множестве параметров служит $l_1$-регуляризация~\cite{Tibshirani1996} и регуляризация ElasticNet~\cite{Hastie2005} для линейных моделей.
Порядок, заданный на множестве значений коэффициентов регуляризации, индуцирует порядок на множестве признаковых описаний и указывает на важность признаков.
В случае нейросетей для регуляризации параметров используется метод исключения параметров~\cite{srivastava2014, molchanov2017}.
Он также задает порядок на множестве параметров модели.

Порядок на множестве параметров нейросети используется не только для удаления неимение релевантных параметров, а и для фиксации параметров в процесе оптимизации параметров. Работа~\cite{grabovoy2020} посвящена оптимизации структуры нейронной сети, а также выбору параметров, которые фиксируются после некоторой итерации градиентного метода. 


\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Предложить байесовский метод выбора моделей с использованием модели учителя с привилегированной и накопленной информацией.
\item Предложить метод назначения априорного распределения параметров модели ученика с использованием апостериорного распределения параметров модели учителя.
\item Предложить вероятностную интерпретацию дистилляции моделей глубокого обучения.
\item Предложить метод использования экспертной информации об исследованной задачи при построении априорного распределения параметров.
\item Предложить метод назначения релевантности параметров моделей глубокого обучения.
\end{enumerate}

\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы вариационного байесовского вывода~\cite{mackay2002,bishop2006}. Вероятностные~\cite{shiriyaev1980} методы к анализу моделей глубокого обучения. Статистические методы~\cite{kobzar2012,bishop2006} анализа распределений параметров моделей глубокого обучения.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
    \item Предложен байесовский метод выбора моделей с использованием модели учителя с привилегированной и накопленной информации.
    \item Доказаны теоремы о свойствах дистилляции, 
    \begin{itemize}
        \item[---] \emph{теоремы об эквивалентности} для дистилляции моделей в случае задачи регрессии и классификации,
        \item[---] \emph{теоремы о виде априорного распределения} параметров модели ученика в байесовской дистилляции.
    \end{itemize}
    \item Предложен метод выравнивания структур параметрических моделей. Предложен метод выбора априорного распределения параметров модели ученика с использованием апостериорного распределения параметров модели учителя для случаев
    \begin{itemize}
        \item[---] различных размерностей пространств параметров отдельных слоев,
        \item[---] различного числа слоев нескольких моделей.
    \end{itemize}
    \item Предложены методы задания порядка на множестве параметров моделей
    \begin{itemize}
        \item[---] на основе корреляции параметров,
        \item[---] на основе оценки скорости сходимости параметров.
    \end{itemize}
    \item Предложена вероятностная интерпретации дистилляции моделей глубокого обучения. Исследованы свойства дистилляции моделей глубокого обучения.
\end{enumerate}

\vspace{0.5cm}
\textbf{Научная новизна.} Разработаны новые подходы к назначению априорного распределения параметров моделей. Предложен метод назначения априорного распределения используя экспертную информацию о задаче. Предложены методы задания порядка на множестве параметров нейросетевых моделей на основе анализа мультиколлиниорности параметров и скорости их сходимости. Предложено вероятностное обобщение дистилляции моделей. Предложено байесовское обобщение дистилляции моделей глубокого обобщения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} Диссертационная работа носит теоретический характер. В работе проводится теоретический анализ методов снижения размерности пространства параметров нейросетевых моделей. Доказаны \emph{теоремы об эквивалентности} для дистилляции моделей в случае задачи регрессии и классификации. Доказаны теоремы об априорном распределения модели для байесовской дистилляции.

\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; снижения пространства параметров моделей глубокого обучения; использование экспертной информации для построения моделей; дистилляция параметрических моделей на основе выравнивания архитектур.

\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
    \item Задача обучения с экспертом для построения интерпретируемых моделей машинного обучения, Международная конференция <<Интеллектуализация обработки информации>>, 2020.
    \item Привилегированная информация и дистилляция моделей, Всероссийская конференция <<63-я научная конференция МФТИ>>, 2020.
    \item Введение отношения порядка на множестве параметров нейронной сети, Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2019.
    \item Анализ априорных распределений в задаче смеси экспертов, Всероссийская конференция <<62-я научная конференция МФТИ>>, 2019.
    \item Поиск оптимальной модели при помощи алгоритмов прореживания, Всероссийская конференция <<61-я научная конференция МФТИ>>, 2018.
    \item Автоматическое определение релевантности параметров нейросети, Международная конференция <<Интеллектуализация обработки информации>>, 2018.
\end{enumerate}

Работа поддержана грантами Российского фонда фундаментальных исследований:

\begin{enumerate}
    \item[1)] 19-07-00875, Развитие методов автоматического построения и выбора вероятностных моделей субоптимальной сложности в задачах глубокого обучения,
    \item[2)] 19-07-01155, Развитие теории порождения моделей локальной аппроксимации для классификации сигналов носимых устройств,
    \item[3)] 19-07-00885, Выбор моделей в задачах декодирования временных рядов высокой размерности.
\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 6 печатных изданиях в журналах, рекомендованных ВАК.
\begin{enumerate}
    \item \textit{Грабовой А.В., Стрижов В.В.} Байесовская дистилляция моделей глубокого обучения~// Автоматика и телемеханика, 2021.
    \item \textit{Грабовой А.В., Стрижов В.В.} Анализ выбора априорного распределения для смеси экспертов~// Журнал вычислительной математики и математической физики, 2021.
    \item \textit{A. Grabovoy, V. Strijov.} Quasi-periodic time series clustering for human // Lobachevskii Journal of Mathematics, 2020.
    \item \textit{Грабовой А.В., Бахтеев О. Ю., Стрижов В.В.} Введение отношения порядка на множестве параметров аппроксимирующих моделей~// Информатика и ее применения, 2020.
    \item \textit{Грабовой А.В., Бахтеев О.Ю., Стрижов В.В.} Определение релевантности параметров нейросети~// Информатика и ее применения, 2019.
    \item \textit{Грабовой А.В., Стрижов В.В.} Вероятностная интерпретация задачи дистилляции~// Автоматика и телемеханика, 2022.
\end{enumerate}

\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.

\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из \total{citnum} наименований. Основной текст занимает \pageref{LastPage} страницы.

\vspace{0.5cm}
\textbf{Краткое содержание работы по главам.}
В главе 1 вводятся основные понятия, поставлены задачи выбора априорного распределения параметров моделей машинного обучения. Проанализированы методы дистилляции и привилегированного обучения предложенные Владимиром Наумовичем Вапником и Джефри Хинтоном. Анализируются существующие методы задания порядка на множестве параметров нейросетевых моделей.
 
В главе 2 предложены методы обобщения дистилляции и привилегированного обучения на основе вероятностного подхода.

В главе 3 предложен байесовский подход для дистилляции моделей глубокого обучения на основе вариационного вывода.

В главе 4 предложены методы задания априорного распределения параметров локальных моделей в задаче обучения смеси экспертов.

В главе 5 предложены методы введения отношения порядка на множестве параметров аппроксимирующих моделей.

В главе 6 проведен анализ прикладных задач, которые используют экспертную информацию.




